{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEW7yBKXO1-3"
      },
      "source": [
        "# Project: Text Classification in PyTorch\n",
        "\n",
        "## Introduction\n",
        "This project deals with neural text classification using PyTorch. Text classification is the process of assigning tags or categories to text according to its content. It's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.\n",
        "\n",
        "Text classification algorithms are at the heart of a variety of software systems that process text data at scale. Email software uses text classification to determine whether incoming mail is sent to the inbox or filtered into the spam folder. Discussion forums use text classification to determine whether comments should be flagged as inappropriate.\n",
        "\n",
        "**_Example:_** A simple example of text classification would be Spam Classification. Considering a bunch of emails that I would receive in the my personal inbox if the email service provider did not have a spam filter algorithm. Because of the spam filter, spam emails get redirected to the Spam folder, while I receive only non-spam (\"_ham_\") emails in your inbox.\n",
        "\n",
        "![](http://blog.yhat.com/static/img/spam-filter.png)\n",
        "\n",
        "## Task\n",
        "Here, I want to focus on a specific type of text classification task, \"Document Classification into Topics\". It can be addressed as classifying text data or even large documents into separate discrete topics/genres of interest.\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*YWEqFeKKKzDiNWy5UfrTsg.png)\n",
        "\n",
        "In this project, I will be working on classifying given text data into discrete topics or genres. A bunch of text data is given, each of which has a label attached. We learn why I think the contents of the documents have been given these labels based on their words. I need to create a neural classifier that is trained on this given information. Once I have a trained classifier, it should be able to predict the label for any new document or text data sample that is fed to it. The labels need not have any meaning to us, nor to you necessarily.\n",
        "\n",
        "## Data\n",
        "There are various datasets that we can use for this purpose. This project shows the usage of the text classification datasets in the PyTorch library ``torchtext``. There are different datasets in this library like `AG_NEWS`, `SogouNews`, `DBpedia`, and others. This project will deal with training a supervised learning algorithm for classification using one of these datasets. In this project, first I will work with the `AG_NEWS` dataset for the Linear Model.\n",
        "\n",
        "## Load Data\n",
        "\n",
        "A bag of **ngrams** feature is applied to capture some partial information about the local word order. In practice, bi-grams or tri-grams are applied to provide more benefits as word groups than only one word.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*\"I love Neural Networks\"*\n",
        "* **Bi-grams:** \"I love\", \"love Neural\", \"Neural Networks\"\n",
        "* **Tri-grams:** \"I love Neural\", \"love Neural Networks\"\n",
        "\n",
        "In the code below, I have loaded the `AG_NEWS` dataset from the ``torchtext.datasets.TextClassification`` package with bi-grams feature. The dataset supports the ngrams method. By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbc6ZFj8O1-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7b64da-2d56-4b5b-8c84-54236a38dd98"
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "!pip install torchtext==0.4\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 23.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 22.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.7.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 71.6MB/s]\n",
            "120000lines [00:06, 17886.47lines/s]\n",
            "120000lines [00:15, 7894.45lines/s]\n",
            "7600lines [00:00, 8055.16lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuN17-hqO1-_"
      },
      "source": [
        "## Model\n",
        "\n",
        "My first simple model is composed of an [`EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and a linear layer.\n",
        "\n",
        "``EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. ``EmbeddingBag`` requires no padding here since the text lengths are saved in offsets. Additionally, since ``EmbeddingBag`` accumulates the average across the embeddings on the fly, ``EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD5q7vqeO1_A"
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    # Defining the __init__() method with proper parameters\n",
        "    # (vocabulary size, dimensions of the embeddings, number of classes)\n",
        "    def __init__(self, VOCAB_SIZE, EMBED_DIM, NUM_CLASS):\n",
        "        super().__init__()\n",
        "        # Defining the embedding layer\n",
        "        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=True)\n",
        "        # Defining the linear forward layer\n",
        "        self.fc = nn.Linear(EMBED_DIM, NUM_CLASS)\n",
        "        # Initializing weights\n",
        "        self.initialize_weights()\n",
        "    # Defining a method to initialize weights.\n",
        "        # The weights should be random in the range of -0.5 to 0.5.\n",
        "        # Initialize bias values as zero.\n",
        "    def initialize_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.fc.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    # Defining the forward function.\n",
        "        # This should calculate the embeddings and return the linear layer\n",
        "        # with calculated embedding values.\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embeddings = self.embedding(text, offsets)\n",
        "        return self.fc(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87HyWVnWO1_E"
      },
      "source": [
        "## Checking the data before I proceed!\n",
        "\n",
        "Okay, so I know that I'm using the `AG_NEWS` dataset in this project, but do not know any specific details. Let's find out!\n",
        "\n",
        "Reported the following:\n",
        "* Vocabulary size (VOCAB_SIZE)\n",
        "* Number of classes (NUM_CLASS)\n",
        "* Names of the classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynw4HqisXBlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5f0f4e-2eb3-4ec1-c518-a69a09a1549a"
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "print(VOCAB_SIZE)\n",
        "\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "print(NUM_CLASS)\n",
        "\n",
        "labels = train_dataset.get_labels()\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1308844\n",
            "4\n",
            "{0, 1, 2, 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeCDRNdXcFR"
      },
      "source": [
        "Vocabulary size = 1308844\n",
        "Number of classes = 4\n",
        "Names of the classes = {0,1,2,3}, which corresponds to world,sports,business and science/tech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGVufXe3O1_F"
      },
      "source": [
        "## Create an instance for my model\n",
        "\n",
        "The vocab size is equal to the length of vocab (including single word and ngrams). The number of classes is equal to the number of labels. Using the parameters which are used to analyze the data to create an instance `model` of my text classifier `TextClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6VXRUxdO1_G"
      },
      "source": [
        "'''\n",
        "Paramters and model instance creation.\n",
        "'''\n",
        "\n",
        "# Instantiating the Vocabulary size and the number of classes\n",
        "# from the training dataset that I loaded.\n",
        "\n",
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "\n",
        "# Instantiating the model with the parameters that I defined above.\n",
        "# Allocating it to the 'device' variable which is available\n",
        "\n",
        "model = TextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nM5N203O1_J"
      },
      "source": [
        "## Generate batch\n",
        "\n",
        "Since the text entries have different lengths, I need to create a custom function to generate data batches and offsets. This function should be passed to the ``collate_fn`` parameter in the ``DataLoader`` call of pyTorch which I will use to create the data later on. The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. ``collate_fn`` is declared as a top level definition.\n",
        "\n",
        "The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``EmbeddingBag``. The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.\n",
        "\n",
        "The function should take batch as an input parameter. Each entry in the batch contains a pair of values of the text and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEhFJaFmO1_K"
      },
      "source": [
        "# Defining the function definition\n",
        "\n",
        "def generate_batch(batch):\n",
        "\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "\n",
        "    return text, offsets, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMalbxDBO1_O"
      },
      "source": [
        "## Defining the train function\n",
        "\n",
        "Here, defining a function which I will use later on in the project to train the model. The outline of the function is something like this -\n",
        "\n",
        "* load the data as batches\n",
        "* iterate over the batches\n",
        "* find the model output for a forward pass\n",
        "* calculate the loss\n",
        "* perform backpropagation on the loss (optimize it)\n",
        "* find the training accuracy\n",
        "\n",
        "In addition to this, I also need to find the total loss and total training accuracy values. Also, the average values of the total loss and total accuracy need to be found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KK-70i2O1_P"
      },
      "source": [
        "def train(train_data):\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # Using the PyTorch DataLoader class to load the data\n",
        "    # into shuffled batches of appropriate sizes into the variable 'data'\n",
        "\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)\n",
        "\n",
        "\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "\n",
        "        # Performing backprop on the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "\n",
        "        # Storing the output of the model in variable 'output'\n",
        "        output = model(text, offsets)\n",
        "\n",
        "\n",
        "        # Defining the 'loss' variable (with respect to 'output' and 'cls').\n",
        "        # Also calculating the total loss in variable 'train_loss'\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "\n",
        "        # Performing the backward propagation on 'loss' and\n",
        "        # optimize it through the 'optimizer' step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # Calculating and storing the total training accuracy\n",
        "        # in the variable 'total_acc'.\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "\n",
        "    # Adjusting the learning rate here using the scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJh07eG6O1_S"
      },
      "source": [
        "## Defining the test function\n",
        "\n",
        "Using the framework of the `train()` function in the previous cell, try to figure out the structure of the test function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4-GAaejO1_T"
      },
      "source": [
        "def test(test_data):\n",
        "\n",
        "    # Initial values of test loss and test accuracy\n",
        "\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    # Using DataLoader class to load the data\n",
        "    # into non-shuffled batches of appropriate sizes.\n",
        "\n",
        "    data = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "\n",
        "    for text, offsets, cls in data:\n",
        "\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "\n",
        "            # Getting the model output\n",
        "            output = model(text, offsets)\n",
        "\n",
        "\n",
        "            # Calculating and adding the loss to find total 'loss'\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "\n",
        "\n",
        "            # Calculating the accuracy and storing it in the 'acc' variable\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "\n",
        "    return loss / len(test_data), acc / len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWoSYAWO1_Z"
      },
      "source": [
        "## Spliting the dataset and run the model\n",
        "\n",
        "The original `AG_NEWS` has no validation dataset. For this reason,the training dataset is needed to split into training and validation sets with a proper split ratio. The `random_split()` function in the torch.utils core PyTorch library is used.\n",
        "\n",
        "* The initial learning rate is 4.0, number of epochs as 5, training data ratio is 0.9.\n",
        "* A proper loss function is used\n",
        "* Defining an Optimization algorithm (SGD)\n",
        "* Defining a scheduler function to adjust the learning rate through epochs (gamma parameter = 0.9).\n",
        "* The loss and accuracy values for both training and validation data sets will be monitored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xokae7GGO1_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31027329-d5a6-40cf-ef93-e7d756f50fef"
      },
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Setting the number of epochs and the learning rate to\n",
        "# their initial values here\n",
        "\n",
        "N_EPOCHS = 5\n",
        "LEARNING_RATE = 4.0\n",
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "# Seting the intial validation loss to positive infinity\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "\n",
        "# Using the CrossEntropy loss function\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "# Using the SGD optimization algorithm with parameters\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "\n",
        "\n",
        "# Using a scheduler function\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "\n",
        "# Splitting the data into train and validation sets using random_split()\n",
        "train_length = int(len(train_dataset) * 0.95)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_length, len(train_dataset) - train_length])\n",
        "\n",
        "\n",
        "# Finishing the rest of the code below\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0263(train)\t|\tAcc: 84.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 90.7%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0120(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.0%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0070(train)\t|\tAcc: 96.3%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.2%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 98.0%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.4%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.7%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h00Q-8oQO1_d"
      },
      "source": [
        "## Let's  check the test loss and test accuracy\n",
        "\n",
        "As I have trained the model and have seen how well it performs on the training and validation datasets. Now, I need to check the model's performance against the test dataset. Using the test dataset as input, the test loss and test accuracy scores of the model are reported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VaCs4AZO1_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973ff9c7-14d7-4936-a376-9d307cd2ac64"
      },
      "source": [
        "# Computing the results (loss and accuracy) on the test data\n",
        "\n",
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0002(test)\t|\tAcc: 89.7%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbNe9xJfO1_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d2cdac-d1cd-4f3e-e208-517e1a2cda25"
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# Predicting the topic of the above given random text (using bigrams)\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(ex_text_str, model, vocab, 2)])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sports' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiNEF112O1_l"
      },
      "source": [
        "The model is tested with a new sample text. Now, feeding some more random examples of similar text (which are related to at least one of the four topics _\"World\", \"Sports\", \"Business\", \"Sci/Tec\"_ of our problem) to the model and checking how the model reacts. Testing 3 such example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIUCmS_2g2j3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0330b5ad-d060-455f-900e-5ee60df604df"
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "str1 = \"Japan is to ask all schools to close from Monday to prevent the spread of the coronavirus, PM Shinzo Abe says\\\n",
        "The closure thought to affect 13 million students will continue until the school year ends in late March.\"\n",
        "\n",
        "str2 = \"Clearview AI, a start-up with a database of more than three billion photographs from Facebook, YouTube and Twitter, has been hacked.\\\n",
        "The attack allowed hackers to gain access to its client list but it said its servers had not been breached.\"\n",
        "\n",
        "str3= \"Global investors were hit with a sixth day of stock market losses on Thursday, as traders responded to the threat of the coronavirus.\\\n",
        "The string of declines pushed indexes in Europe and the US down more than 10% from their recent highs - sending them into so-called correction territory.\"\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# Predicting the topic of the above given random text (using bigrams)\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str1, model, vocab, 2)])#it's a world news\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str2, model, vocab, 2)])#it's technology related news\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str3, model, vocab, 2)])#it's business related news\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'World' news\n",
            "This is a 'Sci/Tec' news\n",
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OreYiWZlP-e"
      },
      "source": [
        "Model predicted correctly for new three test cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmVu0E3oO1_m"
      },
      "source": [
        "Okay, probably the model still works great with the examples those are fed to it in the previous section.\n",
        "##How about twisting the testcase.\n",
        "\n",
        "Let's feed it some more random text data from completely different genres/topics (not belonging to the 4 topics).\n",
        "\n",
        "Of course the predictions will be limited to the four class labels that the model is trained on. I will try to findout the reason why the labels that the model predicted for the given text inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoM0Zbs1nILq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "149ec194-9802-4a1f-f833-c34c037ce664"
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "str1 = \"Local music often appears at karaoke venues, which is on lease from the record labels. Traditional Japanese music differs markedly \\\n",
        "from Western music, as it is often based on the intervals of human breathing rather than on mathematical timing.\"\n",
        "\n",
        "str2 = \"No doubt, life is beautiful and every moment  a celebration of being alive, but one should be always ready to face adversity and challenges. \\\n",
        "A person who has not encountered difficulties in life can never achieve success.Difficulties test the courage, patience, perseverance and \\\n",
        "true character of a human being. Adversity and hardships make a person strong and ready to face the challenges of life with equanimity. \\\n",
        "There is no doubt that there can be no gain without pain. It is only when one toils and sweats it out that success is nourished and sustained\"\n",
        "\n",
        "str3= \"Education is analyzed as being an important role in the society, where the structure of teaching, learning, and environment is \\\n",
        "frequently debated as factor (main) responsible for the development of people. \\\n",
        "This is why education system, and the structure for teaching shall be considered seriously.\"\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "# Predicting the topic of the above given random text (using bigrams)\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str1, model, vocab, 2)])#it was related to music types in japan.\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str2, model, vocab, 2)])#it's related to philosophy of life\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(str3, model, vocab, 2)])#it's related to education news\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n",
            "This is a 'Business' news\n",
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0LJGXTMn1gt"
      },
      "source": [
        "1. Str1 Text decribes about differnt types of music in Japan. But our model classifies it as business news. This is probably due to the worlds such as \"lease\" and \"mathematical timing\" in the text, which is often used in business.\n",
        "\n",
        "2. Str2 is philosophy about life. But our model classified it as business news. We do not clearly understand why, but we think its because of the words such as adversity and challenges , no gain which often comes in business news.\n",
        "\n",
        "3. Str3 is about education system. But our model classified it as Scinec/Tech news. It is may be due to the fact that in most of the science related news they link education. and also words such as \"teaching , lerning and enviornment \" are used in defining new technology , how to learn it and its developement systems. Also the word \"system\" is more often used in technology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUxQ5VFuO1_n"
      },
      "source": [
        "##Room for improvement\n",
        "The model probably has achieved a good accuracy score. However, there may be lots of things that could still be tried to do to improve the classifier model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8aFWKW2xdTG"
      },
      "source": [
        "1.As I have using simple fully connected neyworks I am considering the current state and predictiong the output, which in other sense is not considering the sequence of the word appearence or temporal relation of the words. So I can use different models such as LSTM where sequence of the word matters not just the words.\n",
        "\n",
        "2. I can increase the depth of the neural network and try. Becuse usually in the initial layers only primitive features are learnt. So I can try to increase the depth of the network and test whether model learns complex features.\n",
        "\n",
        "3. I can reduce the step size and increase the number of epochs for the current network to see whether it gives more validation and test accuracy."
      ]
    }
  ]
}